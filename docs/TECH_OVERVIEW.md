# Technical Overview - Klaus News

## Current Implementation Status

**Status:** ~95% backend implemented (including Research & Article Generation), ~80% frontend implemented (Settings feature complete, Microsoft Teams Integration complete, Cooking workflow UI incomplete)
**Last Updated:** 2026-01-27

This document describes **how Klaus News technically solves** the requirements and what is currently implemented from a technical/architectural perspective.

---

## Architecture

Klaus News uses a **containerized three-tier architecture**:

1. **Backend API** (Python/FastAPI) - âœ… Core business logic fully implemented
2. **Frontend UI** (React/TypeScript) - ðŸŸ¡ Post browsing and Settings UI complete, article UI incomplete
3. **Database** (PostgreSQL) - âœ… Full schema implemented and active

All components run in Docker containers orchestrated via Docker Compose.

---

## Tech Stack

### Backend

- **Python 3.11+**: Programming language
- **FastAPI 0.109+**: Web framework
  - âœ… Automatic OpenAPI docs at `/docs`
  - âœ… Pydantic validation
  - âœ… Async/await support
  - âœ… CORS enabled for localhost:5173
- **PostgreSQL 15**: âœ… Database with complete schema and active data
- **SQLAlchemy 2.0**: âœ… ORM with three models (Post, Article, ListMetadata)
- **psycopg2-binary**: âœ… PostgreSQL adapter
- **APScheduler 3.10**: âœ… Background job scheduler (2 jobs running: ingest, archive)
- **httpx 0.26**: âœ… HTTP client for X API and Teams webhook
- **OpenAI 2.15.0+**: âœ… OpenAI Python client (fully configured and functional)
- **Research Models:** gpt-5-search-api (Quick Research), o4-mini (Agentic Research with web_search tool), o3-deep-research (Deep Research)
- **scikit-learn**: âœ… Machine learning utilities

### Frontend

- **React 18**: âœ… UI library with functional components
- **TypeScript 5.3**: âœ… Full type safety across codebase
- **Vite 5.0**: âœ… Build tool with hot reload
- **Axios 1.6**: âœ… HTTP client connected to backend
- **Quill.js 2.0**: âœ… WYSIWYG editor component (built but not integrated)

### Infrastructure

- **Docker**: Container runtime
- **Docker Compose**: Local orchestration
- **Nginx**: Frontend reverse proxy (in production build)

---

## Database Schema (FULLY IMPLEMENTED)

### Posts Table âœ…

Complete schema with all fields populated by background jobs:

```sql
CREATE TABLE posts (
    id SERIAL PRIMARY KEY,
    post_id VARCHAR UNIQUE NOT NULL,
    original_text TEXT NOT NULL,
    author VARCHAR,
    created_at TIMESTAMP NOT NULL,

    -- AI-generated fields (populated during ingestion)
    ai_title VARCHAR,
    ai_summary TEXT,

    -- Categorization (populated during ingestion)
    category VARCHAR,

    -- Scores (computed during ingestion)
    categorization_score FLOAT,  -- AI confidence 0-1
    worthiness_score FLOAT,      -- Quality score 0-1

    -- Grouping (assigned during ingestion via AI title comparison)
    group_id INTEGER,             -- FK to Groups.id

    -- State: Posts inherit archived/selected from their parent Group
    -- (archived and selected fields removed in V-3)

    -- Timestamps
    ingested_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

-- Indexes
CREATE INDEX idx_post_id ON posts(post_id);
CREATE INDEX idx_archived ON posts(archived);
CREATE INDEX idx_group_id ON posts(group_id);
```

**Location:** [backend/app/models/post.py](backend/app/models/post.py)

**Technical Implementation:**
- SQLAlchemy ORM model with full field mapping
- `group_id` assigned via AI semantic title comparison during ingestion
- `worthiness_score` calculated via AI scoring (with algorithmic fallback)
- Archived posts excluded from main queries via `archived=False` filter

---

### ListMetadata Table âœ…

Tracks X API pagination state for each list:

```sql
CREATE TABLE list_metadata (
    id SERIAL PRIMARY KEY,
    list_id VARCHAR UNIQUE NOT NULL,
    last_tweet_id VARCHAR,  -- Stores last fetched tweet ID for since_id param
    enabled BOOLEAN DEFAULT TRUE,  -- NEW: Allow enable/disable without deletion
    list_name VARCHAR,  -- NEW: User-friendly name
    description TEXT,  -- NEW: Optional notes
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

-- Indexes
CREATE INDEX idx_list_id ON list_metadata(list_id);
```

**Location:** [backend/app/models/list_metadata.py](backend/app/models/list_metadata.py)

**Technical Purpose:**
- Prevents refetching same posts across ingestion cycles
- `last_tweet_id` passed as `since_id` to X API v2 endpoint
- X API only returns tweets posted AFTER this ID (non-inclusive)
- Updated after each successful ingestion run

---

### SystemSettings Table âœ…

Stores configuration key-value pairs with validation:

```sql
CREATE TABLE system_settings (
    id SERIAL PRIMARY KEY,
    key VARCHAR UNIQUE NOT NULL,
    value TEXT NOT NULL,
    value_type VARCHAR NOT NULL,  -- 'int', 'float', 'string', 'bool', 'json'
    description TEXT,
    category VARCHAR,  -- 'scheduling', 'filtering', 'system'
    min_value FLOAT,  -- For numeric validation
    max_value FLOAT,  -- For numeric validation
    updated_at TIMESTAMP DEFAULT NOW(),
    updated_by VARCHAR  -- Future: track who changed settings
);

-- Initial settings (8 default configuration values)
INSERT INTO system_settings (key, value, value_type, description, category, min_value, max_value) VALUES
('ingest_interval_minutes', '30', 'int', 'Minutes between post ingestion runs', 'scheduling', 5, 360),
('archive_age_days', '7', 'int', 'Days before archiving unselected posts', 'scheduling', 1, 30),
('archive_time_hour', '3', 'int', 'Hour of day (0-23) to run archival job', 'scheduling', 0, 23),
('posts_per_fetch', '5', 'int', 'Number of posts to fetch per list', 'scheduling', 1, 100),
('worthiness_threshold', '0.6', 'float', 'Minimum score for recommended posts', 'filtering', 0.3, 0.9),
('duplicate_threshold', '0.85', 'float', 'Similarity threshold for duplicate detection', 'filtering', 0.7, 0.95),
('categories', '[{"id":"uuid-1","name":"Major News","description":"Breaking announcements...","order":1},...]', 'json', 'User-defined categories', 'filtering', NULL, NULL),
('category_mismatches', '[]', 'json', 'Log of category matching failures (capped at 100)', 'filtering', NULL, NULL),
('scheduler_paused', 'false', 'bool', 'Whether background scheduler is paused', 'system', NULL, NULL),
('auto_fetch_enabled', 'true', 'bool', 'Enable/disable automatic post fetching', 'scheduling', NULL, NULL);
```

**Location:** [backend/app/models/system_settings.py](backend/app/models/system_settings.py)

**Technical Implementation:**
- Supports multiple value types (int, float, string, bool, json)
- Built-in validation constraints (min_value, max_value)
- Category-based grouping for UI organization
- Tracks update timestamps
- Used by SettingsService with 60-second caching

---

### Prompts Table âœ…

Stores AI prompt configurations for all OpenAI operations:

```sql
CREATE TABLE prompts (
    id SERIAL PRIMARY KEY,
    name VARCHAR(100) UNIQUE NOT NULL,  -- e.g., 'categorization', 'worthiness'
    display_name VARCHAR(200) NOT NULL,  -- e.g., 'Post Categorization'
    prompt_text TEXT NOT NULL,
    model VARCHAR(50) NOT NULL DEFAULT 'gpt-5.1',
    temperature FLOAT NOT NULL DEFAULT 0.5,
    max_tokens INT NOT NULL DEFAULT 100,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

-- Initial prompts (6 default configurations)
-- Auto-seeded on first startup via seed_prompts_if_empty() in main.py
-- Models: gpt-5.1 for quality tasks (articles, titles), gpt-5-mini for simple tasks (scoring, categorization)
```

**Location:** [backend/app/models/prompt.py](backend/app/models/prompt.py)

**Technical Implementation:**
- SQLAlchemy ORM model with full field mapping
- Unique constraint on `name` field prevents duplicates
- All 6 prompts auto-seeded on application startup
- Used by PromptService with caching (similar to SettingsService)

---

### Articles Table âœ…

Stores AI-generated articles with metadata:

```sql
CREATE TABLE articles (
    id SERIAL PRIMARY KEY,
    post_id INTEGER REFERENCES posts(id) NOT NULL,

    title VARCHAR NOT NULL,              -- Extracted from first markdown line
    content TEXT NOT NULL,               -- Full article in markdown
    research_summary TEXT,               -- Placeholder for future research feature
    generation_count INTEGER DEFAULT 1,  -- Incremented on regenerate

    posted_to_teams TIMESTAMP,           -- Null until posted, prevents duplicates

    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);
```

**Location:** [backend/app/models/article.py](backend/app/models/article.py)

**Technical Details:**
- Foreign key to posts table ensures referential integrity
- `generation_count` tracks how many times article was regenerated
- `posted_to_teams` timestamp prevents duplicate Teams posts
- `research_summary` field exists but research not implemented

---

### Groups Table âœ… (V-4 NEW)

Stores news story groups as first-class entities:

```sql
CREATE TABLE groups (
    id SERIAL PRIMARY KEY,
    representative_title VARCHAR NOT NULL,
    representative_summary TEXT,  -- V-2: AI-generated for richer matching
    category VARCHAR NOT NULL,
    first_seen TIMESTAMP NOT NULL,
    post_count INTEGER DEFAULT 1 NOT NULL,
    archived BOOLEAN DEFAULT FALSE NOT NULL,  -- V-2: User archives groups
    selected BOOLEAN DEFAULT FALSE NOT NULL,  -- V-2: User selected for article
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_groups_category ON groups(category);
CREATE INDEX idx_groups_first_seen ON groups(first_seen);
CREATE INDEX idx_groups_archived ON groups(archived);  -- V-16: For active group queries
```

**Location:** backend/app/models/group.py

---

### GroupResearch Table âœ… (Brief V-18, Specs V-18)

Stores AI research output for groups:

```sql
CREATE TABLE group_research (
    id SERIAL PRIMARY KEY,
    group_id INTEGER REFERENCES groups(id) NOT NULL,
    research_mode VARCHAR NOT NULL,      -- 'quick', 'agentic', 'deep'
    original_output TEXT NOT NULL,       -- AI-generated research
    edited_output TEXT,                  -- User-edited version
    sources JSONB,                       -- Source URLs with metadata
    model_used VARCHAR NOT NULL,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);
```

**Location:** TBD: backend/app/models/group_research.py (not yet created)

**Technical Purpose:**
- Stores research output from three modes: Quick (gpt-5-search-api), Agentic (o4-mini + web_search), Deep (o3-deep-research)
- Preserves both original AI output and user edits
- Sources stored as JSONB for clickable links in UI
- Research is optional for article generation

### GroupArticles Table âœ… (Brief V-18, Specs V-18)

Stores generated articles with metadata:

```sql
CREATE TABLE group_articles (
    id SERIAL PRIMARY KEY,
    group_id INTEGER REFERENCES groups(id) NOT NULL,
    research_id INTEGER REFERENCES group_research(id),  -- NULL if generated without research
    style VARCHAR NOT NULL,              -- 'news_brief', 'full_article', 'executive_summary', 'analysis', 'custom'
    prompt_used TEXT NOT NULL,           -- The actual prompt used (for custom or resolved preset)
    content TEXT NOT NULL,               -- The generated article (plain text)
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()   -- Updated on each refinement
);
```

**Location:** TBD: backend/app/models/group_articles.py (not yet created)

**Technical Purpose:**
- Links article to group and optional research
- Stores article style and actual prompt used
- Plain text content (formatted for Teams)
- Refinement updates updated_at timestamp

---

### SystemLog Table âœ… (Production Logging System)

Stores all application logs with historical retention:

```sql
CREATE TABLE system_logs (
    id SERIAL PRIMARY KEY,
    timestamp TIMESTAMP SERVER DEFAULT CURRENT_TIMESTAMP NOT NULL,
    level VARCHAR NOT NULL,             -- DEBUG, INFO, WARNING, ERROR, CRITICAL
    logger_name VARCHAR NOT NULL,       -- e.g., 'klaus_news.x_client'
    message TEXT NOT NULL,
    context TEXT,                       -- JSON for structured data
    exception_type VARCHAR,             -- Exception class name
    exception_message TEXT,             -- Exception message
    stack_trace TEXT,                   -- Full traceback
    correlation_id VARCHAR,             -- Request/job correlation ID
    category VARCHAR                    -- api, scheduler, external_api, database
);

-- Indexes for performance (common query patterns)
CREATE INDEX idx_system_logs_timestamp ON system_logs(timestamp);
CREATE INDEX idx_system_logs_level ON system_logs(level);
CREATE INDEX idx_system_logs_logger_name ON system_logs(logger_name);
CREATE INDEX idx_system_logs_category ON system_logs(category);
CREATE INDEX idx_system_logs_correlation_id ON system_logs(correlation_id);

-- Composite indexes for filtered queries
CREATE INDEX idx_timestamp_level ON system_logs(timestamp, level);
CREATE INDEX idx_category_timestamp ON system_logs(category, timestamp);
```

**Location:** [backend/app/models/system_log.py](backend/app/models/system_log.py)

**Technical Purpose:**
- Centralizes application logging with database persistence
- Captures all operations: API calls, scheduler jobs, external API errors
- Stores structured context (JSON) for rich debugging information
- Preserves full stack traces for exception analysis
- Enables filtering by level, category, and time range
- Supports correlation IDs for tracing related log entries

**Key Features:**
- **7-Day Default Retention:** Configurable via `log_retention_days` setting (7-90 days)
- **Automatic Cleanup:** Daily job at 4 AM removes old logs
- **Separate Connection Pool:** Logging uses dedicated database connection to avoid blocking main app
- **Silent Failure:** Logging errors don't crash application
- **Category-Based Organization:** Groups logs by subsystem (api, scheduler, external_api, database)

**Logging Categories:**
- `api`: API endpoint calls and responses
- `scheduler`: Background job execution (ingestion, archival, cleanup)
- `external_api`: X API, OpenAI API, Teams webhook calls
- `database`: Database operations and connection issues

---

## API Endpoints Implementation Status

### Posts API

**âœ… `GET /api/posts`** - FULLY IMPLEMENTED
- Returns all non-archived posts from database
- Ordered by ingested_at DESC (newest first)
- Includes all AI-generated fields (title, summary, category, scores)
- **Location:** [backend/app/api/posts.py:14-28](backend/app/api/posts.py)

**âœ… `GET /api/posts/recommended`** - FULLY IMPLEMENTED
- Filters posts with `worthiness_score > 0.6`
- Excludes archived (`archived=False`) and selected (`selected=False`) posts
- Groups posts by category: `{"Technology": [posts], "Politics": [posts], ...}`
- Returns dict of category â†’ posts arrays
- **Location:** [backend/app/api/posts.py:31-57](backend/app/api/posts.py)
- **Technical Detail:** Uses SQLAlchemy filter and Python grouping via defaultdict

**âŒ `GET /api/posts/{id}`** - PLACEHOLDER
- Returns `{"post": null}`
- TODO: Query post by ID and return
- **Location:** [backend/app/api/posts.py:60-63](backend/app/api/posts.py)

**âŒ `POST /api/posts/{id}/select`** - REMOVED (V-15)
- Selection now happens at group level via `POST /api/groups/{id}/select/`
- See Groups API section for replacement endpoint

---

### Articles API

**âŒ `GET /api/articles`** - PLACEHOLDER
- Returns `{"articles": []}`
- TODO: Query articles from database
- **Location:** [backend/app/api/articles.py:13-15](backend/app/api/articles.py)

**âœ… `POST /api/articles`** - FULLY IMPLEMENTED
- Receives `post_id` in request body
- Calls `openai_client.generate_article(post.original_text)`
- Extracts title from first markdown line
- Creates Article record with `generation_count=1`
- Returns article object
- **Location:** [backend/app/api/articles.py:18-42](backend/app/api/articles.py)
- **Technical Flow:** Get post â†’ Generate via OpenAI â†’ Parse markdown â†’ Store to DB

**âœ… `PUT /api/articles/{id}`** - FULLY IMPLEMENTED
- Receives `content` in request body
- Updates article's content field
- Returns updated article object
- **Location:** [backend/app/api/articles.py:45-59](backend/app/api/articles.py)
- **Use Case:** User edits article in frontend editor

**âœ… `POST /api/articles/{id}/regenerate`** - FULLY IMPLEMENTED
- Fetches article and related post
- Calls `openai_client.generate_article()` with improved prompt
- Increments `generation_count`
- Updates content and title
- Returns updated article
- **Location:** [backend/app/api/articles.py:62-82](backend/app/api/articles.py)
- **Technical Detail:** Uses same generation function but increments counter

**âœ… `POST /api/teams/send`** - FULLY IMPLEMENTED (multi-channel)
- Calls `teams_service.send_to_teams(articleId, channelName, db)`
- Supports multiple configured channels via `TEAMS_CHANNELS` env var
- Returns `{success: true/false, message/error}`
- **Location:** [backend/app/api/teams.py](backend/app/api/teams.py)
- **Technical Detail:** Channel webhooks never exposed to frontend

**âœ… `GET /api/teams/channels`** - FULLY IMPLEMENTED
- Returns list of configured channel names (no webhook URLs)
- **Location:** [backend/app/api/teams.py](backend/app/api/teams.py)

---

### Settings API

**âœ… `GET /api/settings/`** - FULLY IMPLEMENTED
- Returns all settings grouped by category (scheduling, filtering, system)
- Includes value, type, description, min/max constraints
- **Location:** [backend/app/api/settings.py](backend/app/api/settings.py)

**âœ… `GET /api/settings/{key}`** - FULLY IMPLEMENTED
- Returns single setting by key
- Validates key exists
- **Location:** [backend/app/api/settings.py](backend/app/api/settings.py)

**âœ… `PUT /api/settings/{key}`** - FULLY IMPLEMENTED
- Updates setting value with validation
- Invalidates cache on update
- Triggers dynamic job rescheduling for schedule-related settings
- **Location:** [backend/app/api/settings.py](backend/app/api/settings.py)
- **Technical Detail:** Calls reschedule_ingest_job() when ingest_interval_minutes changes

**Category-specific endpoints (via existing settings API):**
- `GET /api/settings/categories` â€” Returns user-defined categories array
- `PUT /api/settings/categories` â€” Updates categories (validates name immutability, max 20, reserved "Other")
- `GET /api/settings/category_mismatches` â€” Returns mismatch log
- `PUT /api/settings/category_mismatches` â€” Clears log (set to `[]`)

**âœ… `POST /api/settings/batch`** - FULLY IMPLEMENTED
- Updates multiple settings atomically in transaction
- Validates all values before committing
- **Location:** [backend/app/api/settings.py](backend/app/api/settings.py)

**âœ… `POST /api/settings/reset`** - FULLY IMPLEMENTED
- Resets all settings to default values
- Returns updated settings
- **Location:** [backend/app/api/settings.py](backend/app/api/settings.py)

**âœ… `GET /api/settings/validate/{key}`** - FULLY IMPLEMENTED
- Validates value against min/max constraints and type
- Returns validation result without saving
- **Location:** [backend/app/api/settings.py](backend/app/api/settings.py)


**Article Style Prompt Endpoints (NEW - Specs V-19):**
- `GET /api/settings/article-prompts/` â€” Returns all four style prompts (news_brief, full_article, executive_summary, analysis)
- `PUT /api/settings/article-prompts/` â€” Updates style prompts (batch update for all four)

---

### Lists Management API

**âœ… `GET /api/lists/`** - FULLY IMPLEMENTED
- Returns all X lists with metadata (enabled status, last_fetch timestamp)
- **Location:** [backend/app/api/lists.py](backend/app/api/lists.py)

**âœ… `POST /api/lists/`** - FULLY IMPLEMENTED
- Adds new X list to configuration
- Validates list_id format
- **Location:** [backend/app/api/lists.py](backend/app/api/lists.py)

**âœ… `PUT /api/lists/{id}`** - FULLY IMPLEMENTED
- Updates list properties (enabled, list_name, description)
- Changes take effect on next ingestion cycle
- **Location:** [backend/app/api/lists.py](backend/app/api/lists.py)

**âœ… `DELETE /api/lists/{id}`** - FULLY IMPLEMENTED
- Removes list from configuration
- Requires confirmation in UI
- **Location:** [backend/app/api/lists.py](backend/app/api/lists.py)

**âœ… `POST /api/lists/{id}/test`** - FULLY IMPLEMENTED
- Tests list connectivity to X API
- Returns validation result (valid/error with details)
- **Location:** [backend/app/api/lists.py](backend/app/api/lists.py)
- **Technical Detail:** Makes actual X API call to validate list exists

**âœ… `GET /api/lists/{id}/stats`** - FULLY IMPLEMENTED
- Returns fetch statistics for specific list
- Shows post count and last fetch timestamp
- **Location:** [backend/app/api/lists.py](backend/app/api/lists.py)

**âœ… `GET /api/lists/export`** - FULLY IMPLEMENTED (v2.0)
- Returns all lists as downloadable JSON file
- Format: `{"export_version": "2.0", "exported_at": "...", "lists": [...]}`
- **Location:** [backend/app/api/lists.py](backend/app/api/lists.py)

**âœ… `POST /api/lists/import`** - FULLY IMPLEMENTED (v2.0)
- Accepts JSON file upload, validates schema, imports lists
- Merge behavior: Updates existing list_id, adds new lists
- Imported lists set to enabled: false by default (safety)
- **Location:** [backend/app/api/lists.py](backend/app/api/lists.py)

---

### Prompts Management API (v2.0 NEW)

**âœ… `GET /api/prompts`** - FULLY IMPLEMENTED
- Returns all 6 prompts with current values
- Includes: name, display_name, prompt_text, model, temperature, max_tokens
- **Location:** [backend/app/api/prompts.py](backend/app/api/prompts.py)

**âœ… `GET /api/prompts/{name}`** - FULLY IMPLEMENTED
- Returns single prompt details by name (e.g., 'categorization', 'worthiness')
- **Location:** [backend/app/api/prompts.py](backend/app/api/prompts.py)

**âœ… `PUT /api/prompts/{name}`** - FULLY IMPLEMENTED
- Updates prompt configuration (text, model, temperature, max_tokens)
- Invalidates PromptService cache
- Changes take effect immediately (next API call uses new prompt)
- **Location:** [backend/app/api/prompts.py](backend/app/api/prompts.py)

**âœ… `POST /api/prompts/{name}/reset`** - FULLY IMPLEMENTED
- Resets prompt to hardcoded default from code
- Shows confirmation dialog in UI
- **Location:** [backend/app/api/prompts.py](backend/app/api/prompts.py)

**âœ… `GET /api/prompts/export`** - FULLY IMPLEMENTED
- Returns all prompts as downloadable JSON file
- Format: `{"export_version": "2.0", "exported_at": "...", "prompts": {"categorization": {...}, ...}}`
- **Location:** [backend/app/api/prompts.py](backend/app/api/prompts.py)

**âœ… `POST /api/prompts/import`** - FULLY IMPLEMENTED
- Accepts JSON file upload, validates schema, imports prompts
- Overwrite behavior: Replaces all matching prompts
- Partial import supported (only updates included prompts)
- **Location:** [backend/app/api/prompts.py](backend/app/api/prompts.py)

---

### Admin Operations API

**âœ… `POST /api/admin/trigger-ingest`** - FULLY IMPLEMENTED
- Manually triggers data ingestion job
- Returns progress updates and result summary
- **Location:** [backend/app/api/admin.py](backend/app/api/admin.py)
- **Technical Detail:** Executes ingest_posts_job synchronously

**âœ… `POST /api/admin/trigger-archive`** - FULLY IMPLEMENTED
- Manually triggers post archival job
- Returns count of archived posts
- **Location:** [backend/app/api/admin.py](backend/app/api/admin.py)

**âœ… `POST /api/admin/pause-scheduler`** - FULLY IMPLEMENTED
- Pauses background scheduler (stops automatic ingestion and archival)
- Sets scheduler_paused setting to TRUE
- **Location:** [backend/app/api/admin.py](backend/app/api/admin.py)

**âœ… `POST /api/admin/resume-scheduler`** - FULLY IMPLEMENTED
- Resumes background scheduler
- Sets scheduler_paused setting to FALSE
- **Location:** [backend/app/api/admin.py](backend/app/api/admin.py)

**âœ… `GET /api/admin/scheduler-status`** - FULLY IMPLEMENTED
- Returns scheduler state (running/paused)
- Shows next run times for scheduled jobs
- **Location:** [backend/app/api/admin.py](backend/app/api/admin.py)

**âœ… `GET /api/admin/system-stats`** - FULLY IMPLEMENTED
- Returns database counts (posts, articles, lists)
- Shows last operation timestamps
- **Location:** [backend/app/api/admin.py](backend/app/api/admin.py)

---

### Groups API (V-14 Updated)

**âœ… `GET /api/groups/`** - FULLY IMPLEMENTED
- Returns all active groups (archived=false)
- Ordered by first_seen DESC
- **Location:** backend/app/api/groups.py

**âœ… `GET /api/groups/archived/`** - FULLY IMPLEMENTED (V-14)
- Returns all archived groups
- Ordered by first_seen DESC
- **Location:** backend/app/api/groups.py

**âœ… `GET /api/groups/{id}/posts/`** - FULLY IMPLEMENTED
- Returns all posts belonging to a specific group
- Ordered by created_at DESC
- **Location:** backend/app/api/groups.py

**âœ… `POST /api/groups/{id}/archive/`** - FULLY IMPLEMENTED (V-14)
- Sets group.archived = true
- Group hidden from active views but remains for matching
- **Location:** backend/app/api/groups.py

**âœ… `POST /api/groups/{id}/unarchive/`** - FULLY IMPLEMENTED (V-14)
- Sets group.archived = false
- Group restored to active views with all posts
- **Location:** backend/app/api/groups.py

**âœ… `POST /api/groups/{id}/select/`** - FULLY IMPLEMENTED (V-14)
- Sets group.selected = true
- All posts in group become source material for article generation
- **Location:** backend/app/api/groups.py

---

### Logs API (Production Monitoring System)

**âœ… `GET /api/logs/`** - FULLY IMPLEMENTED
- Returns paginated system logs with filtering
- Query parameters:
  - `level`: Filter by log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
  - `category`: Filter by category (api, scheduler, external_api, database)
  - `logger_name`: Filter by specific logger
  - `search`: Search in message text
  - `hours`: Time window in hours (default 24, max 168)
  - `limit`: Maximum logs to return (default 100, max 1000)
  - `offset`: Pagination offset
- Returns: `{logs: [...], total: int, offset: int, limit: int}`
- Ordered by timestamp DESC (newest first)
- **Location:** [backend/app/api/logs.py:14-92](backend/app/api/logs.py)
- **Technical Detail:** Uses composite indexes for efficient filtering

**âœ… `GET /api/logs/stats`** - FULLY IMPLEMENTED
- Returns log statistics for dashboard display
- Query parameters:
  - `hours`: Time window in hours (default 24, max 168)
- Returns:
  - `time_window_hours`: Configured time window
  - `total_logs`: Total count in window
  - `error_count`: ERROR + CRITICAL count
  - `by_level`: Dict of counts per level
  - `by_category`: Dict of counts per category
- **Location:** [backend/app/api/logs.py:95-137](backend/app/api/logs.py)
- **Use Case:** Powers stats cards in frontend UI

**âœ… `GET /api/logs/{log_id}`** - FULLY IMPLEMENTED
- Returns full log details including stack trace
- Includes all fields: timestamp, level, logger, message, context, exception details, correlation ID
- Returns 404 if log not found
- **Location:** [backend/app/api/logs.py:140-172](backend/app/api/logs.py)
- **Use Case:** Powers log detail modal

**âœ… `DELETE /api/logs/cleanup`** - FULLY IMPLEMENTED
- Deletes logs older than specified days
- Query parameters:
  - `days`: Number of days to retain (min 7, max 90, default 30)
- Returns: `{message: str, deleted_count: int}`
- **Location:** [backend/app/api/logs.py:175-199](backend/app/api/logs.py)
- **Technical Detail:** Respects 7-day minimum to prevent accidental data loss

---

### Teams API (NEW)

**`GET /api/teams/channels`** - FULLY IMPLEMENTED
- Returns list of configured channel names (no webhook URLs)
- Response: `{"channels": [{"name": "general-news"}, ...]}`
- Returns empty array if TEAMS_CHANNELS not configured
- **Location:** backend/app/api/teams.py

**`POST /api/teams/send`** - FULLY IMPLEMENTED
- Sends article to specified Teams channel
- Request: `{"articleId": "uuid", "channelName": "tech-updates"}`
- Success response: `{"success": true, "message": "Article sent to #tech-updates"}`
- Error responses: "Channel not found", "Article not found", "Failed to send to Teams"
- **Location:** backend/app/api/teams.py

---

### Research API (NEW - Specs V-19)

**âœ… `POST /api/groups/{id}/research/`** - READY FOR IMPLEMENTATION
- Accepts research_mode parameter ('quick', 'agentic', 'deep')
- Calls OpenAI with appropriate model and tools
- Stores result in group_research table
- Returns research output with sources
- **Location:** TBD: backend/app/api/research.py

**âœ… `GET /api/groups/{id}/research/`** - READY FOR IMPLEMENTATION
- Returns current research for group
- Includes both original and edited output
- Returns null if no research exists
- **Location:** TBD: backend/app/api/research.py

**âœ… `PUT /api/groups/{id}/research/`** - READY FOR IMPLEMENTATION
- Accepts edited_output parameter
- Updates group_research.edited_output
- Preserves original_output
- **Location:** TBD: backend/app/api/research.py

### Article Generation API (NEW - Specs V-19)

**âœ… `POST /api/groups/{id}/article/`** - READY FOR IMPLEMENTATION
- Accepts style parameter ('news_brief', 'full_article', 'executive_summary', 'analysis', 'custom')
- Accepts optional custom_prompt if style='custom'
- Accepts optional research_id to include research context
- Resolves prompt template from settings (article_prompt_{style})
- Calls OpenAI to generate article
- Stores result in group_articles table
- Returns generated article
- **Location:** TBD: backend/app/api/articles.py (extend existing)

**âœ… `GET /api/groups/{id}/article/`** - READY FOR IMPLEMENTATION
- Returns current article for group
- Returns null if no article exists
- **Location:** TBD: backend/app/api/articles.py (extend existing)

**âœ… `PUT /api/groups/{id}/article/refine/`** - READY FOR IMPLEMENTATION
- Accepts instruction parameter (user refinement request)
- Uses current article + research + posts + instruction as context
- Generates refined version via OpenAI
- Replaces article content (no draft history)
- Updates updated_at timestamp
- Returns refined article
- **Location:** TBD: backend/app/api/articles.py (extend existing)

---

### Health Checks

**âœ… `GET /health`** - Returns `{"status": "healthy", "app": "Klaus News"}`
**âœ… `GET /`** - Returns welcome message

**Location:** [backend/app/main.py](backend/app/main.py)

---

## Service Implementations (FULLY FUNCTIONAL)

### X API Client âœ…

**Location:** [backend/app/services/x_client.py](backend/app/services/x_client.py)

**Technical Implementation:**

```python
class XClient:
    def __init__(self):
        self.api_key = settings.x_api_key
        # TODO: Initialize httpx client (works inline currently)

    async def fetch_posts_from_list(
        self,
        list_id: str,
        since_id: str = None,
        max_results: int = 5
    ) -> list[dict]:
        """Fetches posts from X list via API v2"""
```

**How It Works:**
1. Constructs X API v2 URL: `https://api.twitter.com/2/lists/{list_id}/tweets`
2. Query params:
   - `max_results=5` (per requirements)
   - `tweet.fields=created_at,author_id` (metadata)
   - `expansions=author_id` (resolve author username)
   - `since_id={last_tweet_id}` (pagination, prevents refetch)
3. Makes async httpx GET request with Bearer token auth
4. Parses response JSON
5. Resolves author usernames from `includes.users` array
6. Returns list of dicts: `[{id, text, author, created_at}, ...]`

**Error Handling:**
- Returns empty list on non-200 status
- Handles missing expansions gracefully

**Pagination Strategy:**
- Uses X API v2's `since_id` parameter
- Fetches only tweets posted AFTER last_tweet_id
- Non-inclusive (doesn't refetch the since_id tweet)
- Solves "Problem 1: Refetch Prevention" from requirements

---

### OpenAI Client âœ…

**Location:** [backend/app/services/openai_client.py](backend/app/services/openai_client.py)

**Technical Implementation:**

```python
class OpenAIClient:
    def __init__(self):
        self.api_key = settings.openai_api_key
        self.model = "gpt-5.1"  # Default model for quality tasks
        # Uses AsyncOpenAI for async API calls
```

**API Parameter Requirements (CRITICAL):**

All OpenAI API calls follow these patterns for newer models (gpt-5.x series):

1. **Use `max_completion_tokens` instead of `max_tokens`:**
   ```python
   # CORRECT for gpt-5.1, gpt-5-mini, gpt-5.2
   response = await client.chat.completions.create(
       model="gpt-5.1",
       messages=[...],
       max_completion_tokens=100  # NOT max_tokens
   )
   ```

2. **Temperature is model-dependent:**
   - `gpt-5.1`, `gpt-5.2`: Support custom temperature (0.0-2.0)
   - `gpt-5-mini`: Reasoning model, only supports `temperature=1` (default) - **do not pass temperature parameter**

   ```python
   # Conditional temperature handling
   create_kwargs = {
       "model": model,
       "messages": messages,
       "max_completion_tokens": max_tokens
   }
   if "temperature" in prompt_config:  # Only add if supported
       create_kwargs["temperature"] = prompt_config["temperature"]
   response = await client.chat.completions.create(**create_kwargs)
   ```

3. **All methods log to system_logs table:**
   - Logger: `klaus_news.openai_client`
   - Category: `external_api`
   - Logs: method name, model, prompt snippet, response status, token counts, errors

**Three Fully Functional Methods:**

#### 1. `generate_title_and_summary(post_text: str)` âœ…

**How It Works:**
- **Title Generation:**
  - Prompt: "Generate a concise title (max 100 chars)..."
  - Temperature: 0.5 (balanced creativity)
  - Max tokens: 30
  - Returns: Short, clear headline

- **Summary Generation:**
  - Prompt: "Summarize in 2-3 sentences..."
  - Temperature: 0.5
  - Max tokens: 100
  - Returns: Objective overview

- **Technical Detail:** Makes two separate OpenAI API calls (one for title, one for summary)
- **Error Handling:** Returns fallback values on API failure

#### 2. `categorize_post(post_text: str)` âœ…

**How It Works:**
- Prompt: Uses `{{CATEGORIES}}` placeholder; at runtime, assembles categories from `categories` setting plus hardcoded 'Other'
- Model: GPT-5-mini (cost-effective for simple classification)
- Temperature: 0.3 (conservative, consistent)
- Max tokens: 10
- Logprobs: True (to extract confidence)
- Returns: `{"category": "Technology", "confidence": 0.92}`

**Confidence Score Extraction:**
- Parses `logprobs` from response
- Converts log probability to 0-1 score
- Fallback: 0.8 if logprobs unavailable

**Technical Detail:** Uses structured output to ensure category matches enum

#### 7. `build_categorization_prompt()` âœ…

**How It Works:**
- Retrieves prompt skeleton from `prompts.categorize_post`
- Loads user categories from `categories` setting
- Sorts categories by `order` field
- Formats as bullet list: `- {name}: {description}`
- Appends hardcoded "Other: Posts that don't clearly fit the above categories"
- Replaces `{{CATEGORIES}}` placeholder in skeleton

**Returns:** Complete prompt string ready for OpenAI API

#### 8. `match_category(ai_response, valid_categories)` âœ…

**How It Works:**
- Step 1: Exact match (case-insensitive) â€” returns matched category
- Step 2: Partial match (substring in either direction) â€” returns matched category
- Step 3: No match â€” returns "Other", triggers mismatch logging

**Returns:** Tuple (matched_category, was_exact_match)

#### 9. `log_category_mismatch(ai_response, valid_categories, post_text)` âœ…

**How It Works:**
- Retrieves `category_mismatches` setting
- Appends new entry: timestamp, ai_response, valid_categories, post_snippet (first 100 chars), assigned_category
- Caps at 100 entries (removes oldest if exceeded)
- Saves back to `category_mismatches` setting

**Purpose:** Monitors AI categorization accuracy; surfaced in Settings UI

#### 3. `generate_article(post_text: str, research_summary: str = "")` âœ…

**How It Works:**
- Prompt requirements:
  - Informative headline
  - 3-5 paragraphs
  - Provide context/background
  - Objective tone
  - Markdown formatted
- Model: GPT-5.1 (quality model for article generation)
- Temperature: 0.7 (creative but coherent)
- Max tokens: 1000
- Returns: Full markdown article

**Research Integration:**
- `research_summary` parameter exists but not used (research not implemented)
- Future: Could integrate web search or additional context

#### 4. `score_worthiness(post_text: str, category: str)` âœ… (v2.0 NEW)

**How It Works:**
- Prompt: Evaluates newsworthiness for internal company newsletter
- Criteria: Relevance (40%), Quality (40%), Timeliness (20%)
- Model: GPT-5-mini (cost-effective for scoring, configurable via PromptService)
- Temperature: 0.3 (low for consistency)
- Max tokens: 10
- Returns: Float between 0.0 and 1.0
- Error handling: Returns 0.5 (neutral score) if API fails or returns non-numeric

**Technical Detail:** Replaces algorithmic scoring formula that was in scheduler.py

**Fallback Strategy:**
- If AI call fails, falls back to algorithmic calculation:
  - `worthiness = 0.4 Ã— relevance + 0.4 Ã— quality + 0.2 Ã— recency`
  - Prevents ingestion from breaking due to API issues

#### 5. `detect_duplicate(new_post_text: str, existing_post_text: str)` âœ…

**How It Works:**
- Compares two posts for semantic similarity
- Model: GPT-5-mini (cost-effective for similarity checks, configurable via PromptService)
- Temperature: 0.0 (deterministic)
- Returns: Float (0.0-1.0 similarity score)
- Higher score = more similar

**Note:** This method exists but is not used in the current ingestion flow. The scheduler uses `compare_titles_semantic()` instead.

#### 6. `compare_titles_semantic(new_title: str, existing_title: str)` âœ…

**How It Works:**
- Compares two AI-generated titles for semantic similarity
- Model: GPT-5-mini (cost-effective)
- Prompt: Asks AI to rate similarity from 0.0 to 1.0
- Returns: Float (0.0 = different topics, 1.0 = same story)
- Result compared against `duplicate_threshold` setting (default 0.85)

**Technical Detail:** Called during ingestion; only compares posts in same category within last 7 days, limited to 50 candidates for cost control.

#### Research Methods (NEW - Specs V-17)

**Location:** backend/app/services/openai_client.py (extend existing)

**Three Research Methods to Implement:**

##### 1. `quick_research(topic: str, posts: list)` âœ… READY
- **Model:** gpt-5-search-api
- **Tools:** web_search
- **How It Works:** Single search pass, returns basic facts
- **Speed:** Fast (seconds)
- **Cost:** $ (low)
- **Implementation:**
```python
response = client.responses.create(
    model="gpt-5-search-api",
    tools=[{"type": "web_search"}],
    input=research_prompt
)
```

##### 2. `agentic_research(topic: str, posts: list)` âœ… READY
- **Model:** o4-mini
- **Tools:** web_search
- **How It Works:** Model reasons, searches iteratively, decides when done
- **Speed:** Medium (30s-2min)
- **Cost:** $$ (medium)
- **Implementation:**
```python
response = client.responses.create(
    model="o4-mini",
    tools=[{"type": "web_search"}],
    input=research_prompt
)
```

##### 3. `deep_research(topic: str, posts: list)` âœ… READY
- **Model:** o3-deep-research
- **Tools:** (built-in)
- **How It Works:** Exhaustive multi-source investigation, hundreds of sources
- **Speed:** Slow (minutes)
- **Cost:** $$$ (high)
- **Implementation:**
```python
response = client.responses.create(
    model="o3-deep-research",
    input=research_prompt
)
```

---

### Duplicate Detection âœ…

**Location:** Implemented directly in [backend/app/services/scheduler.py](backend/app/services/scheduler.py) and [backend/app/services/openai_client.py](backend/app/services/openai_client.py)

**Technical Implementation - AI-Only Approach:**

Posts are grouped using AI semantic comparison against Group representative titles during ingestion (V-17):

1. **Generate AI Title:** New post gets AI-generated title via `generate_title_and_summary()`

2. **Filter Candidates:** Query existing Groups matching (V-4/V-17):
   - Same category as new post
   - Include ALL groups (archived and non-archived) - prevents duplicate groups when topics resurface
   - No time filter on Groups (they persist)
   - Compare against group.representative_title

3. **AI Comparison:** For each candidate Group:
   - Call `compare_titles_semantic(new_title, group.representative_title)`
   - AI returns similarity score (0.0-1.0)
   - If score >= `duplicate_threshold` (default 0.85) â†’ match found

4. **Group Assignment:**
   - If match found â†’ assign group.id, increment group.post_count; group stays in current state (archived or not per V-5)
   - If no match â†’ create new Group record with representative_title, representative_summary, category, post_count=1, archived=false, selected=false

**Configuration:**
- `duplicate_threshold` setting controls minimum AI confidence required (default: 0.85)
- Configurable in Settings UI under "Duplicate Detection"
- Lower threshold = more aggressive grouping
- Higher threshold = stricter matching

**Technical Achievement:**
- Solves "Problem 2: Topic Grouping" from requirements
- AI-powered semantic understanding catches rephrased content
- Configurable sensitivity via threshold setting
- Cost-optimized via category filtering and candidate limits

---

### Scoring Algorithm âœ…

**Location:** [backend/app/services/scoring.py](backend/app/services/scoring.py)

**Technical Implementation:**

```python
def calculate_worthiness_score(
    post: dict,
    categorization_score: float,
    post_age_days: float
) -> float:
    """Calculate worthiness score (0-1)"""
```

**Formula:**
```
worthiness = 0.4 Ã— relevance + 0.4 Ã— quality + 0.2 Ã— recency
```

**Component Breakdown:**

1. **Relevance (40%):**
   - Uses `categorization_score` (AI confidence from categorization)
   - Higher confidence = more relevant

2. **Quality (40%):**
   - Length score: `1 / (1 + e^(-(len - 100) / 50))` (sigmoid function)
     - Optimal length: ~100 chars
     - Too short or too long = lower score
   - Coherence score:
     - Excessive caps: `CAPS_ratio < 0.5` (all caps = low quality)
     - Punctuation: Has punctuation? +0.5 bonus
   - Quality = `0.7 Ã— length_score + 0.3 Ã— coherence_score`

3. **Recency (20%):**
   - Linear decay over 7 days: `max(0, 1 - (age_days / 7))`
   - New post (day 0): recency = 1.0
   - 7 days old: recency = 0.0

**Technical Rationale (v2.0 AI-First Approach):**
- **Primary:** AI-based scoring via OpenAI worthiness prompt
  - Evaluates relevance, quality, timeliness holistically
  - More nuanced than algorithmic formula
  - Configurable via Settings â†’ Prompts tab
- **Fallback:** Algorithmic calculation if AI fails:
  - Formula: `0.4 Ã— relevance + 0.4 Ã— quality + 0.2 Ã— recency`
  - Relevance: Uses categorization_score (AI confidence)
  - Quality: Length score (sigmoid) + coherence (caps ratio, punctuation)
  - Recency: Linear decay over 7 days
- **Error Handling:** Default to 0.5 (neutral) if both AI and algorithm fail
- Threshold 0.6 used for "Recommended" filter (unchanged)

---

### Teams Integration

**Location:** backend/app/services/teams_service.py

**Technical Implementation:**

TeamsService supports multi-channel configuration via `TEAMS_CHANNELS` environment variable (JSON array of `{name, webhookUrl}` objects).

**Adaptive Card v1.4 Format:**
```json
{
  "type": "message",
  "attachments": [{
    "contentType": "application/vnd.microsoft.card.adaptive",
    "content": {
      "$schema": "http://adaptivecards.io/schemas/adaptive-card.json",
      "type": "AdaptiveCard",
      "version": "1.4",
      "body": [
        {"type": "TextBlock", "text": "CATEGORY", "size": "small", "color": "accent", "weight": "bolder"},
        {"type": "TextBlock", "text": "Article Title", "size": "large", "weight": "bolder", "wrap": true},
        {"type": "TextBlock", "text": "Summary text...", "wrap": true},
        {"type": "TextBlock", "text": "Source: TechCrunch â€¢ Jan 26, 2026", "size": "small", "isSubtle": true}
      ],
      "actions": [
        {"type": "Action.OpenUrl", "title": "Read Article", "url": "https://..."}
      ]
    }
  }]
}
```

**How It Works:**
1. Frontend calls GET /api/teams/channels to fetch available channels
2. User selects channel in TeamsChannelModal
3. Frontend calls POST /api/teams/send with articleId and channelName
4. Backend builds Adaptive Card from article data
5. Backend POSTs to channel's webhook URL
6. Success/error response returned to frontend

---

### Background Scheduler âœ…

**Location:** [backend/app/services/scheduler.py](backend/app/services/scheduler.py)

**Technical Implementation:**

Uses APScheduler with AsyncIOScheduler for async tasks.

#### Job 1: `ingest_posts_job()` âœ…

**Schedule:** Configurable (default: every 30 minutes, range: 5 minutes to 6 hours)
**Configuration:** `scheduler.add_job(ingest_posts_job, 'interval', minutes=settings.get('ingest_interval_minutes', 30))`
**Dynamic Rescheduling:** Job is rescheduled via `reschedule_ingest_job()` when ingest_interval_minutes setting changes (no restart required)

**How It Works:**
1. Get configured X list IDs from settings
2. For each list:
   - Query ListMetadata for `last_tweet_id`
   - Call `x_client.fetch_posts_from_list(list_id, since_id=last_tweet_id, max_results=5)`
   - For each new post (v2.0 updated flow):
     - Check if `post_id` already exists (skip if duplicate)
     - Check `auto_fetch_enabled` setting; if false, skip ingestion and return early
     - Call `openai_client.categorize_post()` â†’ get category + confidence
     - Call `openai_client.generate_title_and_summary()` â†’ get title + summary
     - Call `openai_client.score_worthiness(post_text, category)` â†’ get AI worthiness score
       - Falls back to algorithmic calculation if AI fails
     - Assign `group_id` via Group-based AI title comparison (V-17):
       1. Query ALL Groups where category = new_category (including archived groups)
       2. Call `openai_client.compare_titles_semantic(new_title, group.representative_title)` for each group
       3. If similarity >= `duplicate_threshold` â†’ assign to existing group, increment group.post_count
       4. Otherwise â†’ create new Group record with representative_title, representative_summary, archived=false, selected=false
     - Insert to database as new Post record
   - Update ListMetadata: `last_tweet_id = max(fetched_post_ids)`
3. Commit transaction

**Technical Flow:**
```
X API â†’ Categorize (OpenAI) â†’ Generate Title/Summary (OpenAI)
  â†’ Score â†’ AI Duplicate Check â†’ Group â†’ Database
```

**Error Handling:**
- Skips existing post_ids (upsert-like behavior)
- Continues on individual post failure
- Logs errors but doesn't crash scheduler

#### Job 2: `archive_posts_job()` âœ…

**Schedule:** Daily at 3:00 AM UTC
**Configuration:** `scheduler.add_job(archive_posts_job, 'cron', hour=3)`

**How It Works:**
1. Calculate cutoff date: `now - 7 days`
2. Query posts where:
   - `selected = False` (user didn't select)
   - `archived = False` (not already archived)
   - `ingested_at < cutoff` (older than 7 days)
3. Bulk update: Set `archived = True`
4. Commit transaction

**Technical Details:**
- Uses SQLAlchemy bulk update (efficient)
- Doesn't delete posts (soft delete via archived flag)
- Archived posts excluded from API queries via `archived=False` filter

#### Job 3: `cleanup_logs_job()` âœ…

**Schedule:** Daily at 4:00 AM UTC
**Configuration:** `scheduler.add_job(cleanup_logs_job, 'cron', hour=4, id='cleanup_logs')`

**How It Works:**
1. Read `log_retention_days` setting (default: 7 days)
2. Check if scheduler is paused (skip if paused)
3. Calculate cutoff date: `now - retention_days`
4. Delete all logs where `timestamp < cutoff_date`
5. Log completion with deleted count
6. Commit transaction

**Technical Details:**
- Uses SQLAlchemy DELETE with WHERE clause (efficient)
- Permanently deletes old logs (not soft delete)
- Logs its own execution (meta-logging)
- Respects scheduler pause state
- Prevents unbounded database growth

---

### Logging Infrastructure âœ…

**Location:** [backend/app/services/logging_handler.py](backend/app/services/logging_handler.py), [backend/app/services/logging_config.py](backend/app/services/logging_config.py)

**Technical Implementation:**

#### DatabaseLogHandler (Custom Logging Handler)

```python
class DatabaseLogHandler(logging.Handler):
    """Custom logging handler that writes logs to system_logs table"""

    def __init__(self, category=None):
        super().__init__()
        self.category = category
        # Use separate engine for logging to avoid interference with main app
        self.engine = create_engine(settings.database_url, pool_pre_ping=True)
        self.SessionLocal = sessionmaker(bind=self.engine)

    def emit(self, record):
        """Write log record to database"""
        try:
            from app.models.system_log import SystemLog

            db = self.SessionLocal()
            try:
                # Extract exception info if present
                exception_type = None
                exception_message = None
                stack_trace = None

                if record.exc_info:
                    exc_type, exc_value, exc_tb = record.exc_info
                    exception_type = exc_type.__name__ if exc_type else None
                    exception_message = str(exc_value) if exc_value else None
                    stack_trace = ''.join(traceback.format_exception(exc_type, exc_value, exc_tb))

                # Build context from extra fields
                context = {}
                for key, value in record.__dict__.items():
                    if key not in ['name', 'msg', 'args', 'created', ...]:  # Filter standard fields
                        try:
                            json.dumps(value)  # Test serializability
                            context[key] = value
                        except (TypeError, ValueError):
                            context[key] = str(value)

                log_entry = SystemLog(
                    timestamp=datetime.utcnow(),
                    level=record.levelname,
                    logger_name=record.name,
                    message=record.getMessage(),
                    context=json.dumps(context) if context else None,
                    exception_type=exception_type,
                    exception_message=exception_message,
                    stack_trace=stack_trace,
                    correlation_id=context.get('correlation_id'),
                    category=self.category or context.get('category')
                )

                db.add(log_entry)
                db.commit()
            finally:
                db.close()
        except Exception:
            # Silently fail - logging should never break the application
            self.handleError(record)
```

**How It Works:**
- Extends Python's built-in `logging.Handler` class
- Uses **separate database connection pool** to avoid blocking main application
- Extracts exception info (type, message, stack trace) from log records
- Captures **extra fields** as JSON context (e.g., `logger.info("msg", extra={'foo': 'bar'})`)
- Silently fails on errors (prevents logging from crashing app)
- Writes to `system_logs` table in separate transaction

**Key Design Decisions:**
- **Separate Connection Pool:** Prevents logging from exhausting main app's database connections
- **Silent Failure:** Uses `handleError()` to log errors without raising exceptions
- **Context Extraction:** Filters standard log record fields, only stores extra user-provided context
- **JSON Serializability:** Tests each context value, converts non-JSON types to strings

#### logging_config (Logger Setup)

```python
def setup_logging():
    """Configure application logging with database handler"""

    # Root logger configuration
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[logging.StreamHandler()]  # Keep console output
    )

    # Add database handler to specific loggers
    loggers_config = [
        ('klaus_news.x_client', 'external_api'),
        ('klaus_news.openai_client', 'external_api'),
        ('klaus_news.teams_service', 'external_api'),
        ('klaus_news.scheduler', 'scheduler'),
        ('klaus_news.api', 'api'),
        ('klaus_news.database', 'database'),
    ]

    for logger_name, category in loggers_config:
        logger = logging.getLogger(logger_name)
        logger.setLevel(logging.INFO)
        db_handler = DatabaseLogHandler(category=category)
        db_handler.setLevel(logging.INFO)
        logger.addHandler(db_handler)

    return logging.getLogger('klaus_news')
```

**How It Works:**
- Configures root logger with console output (keeps docker logs visible)
- Attaches `DatabaseLogHandler` to specific subsystem loggers
- Each logger gets category label (api, scheduler, external_api, database)
- Maintains console output + database persistence (dual logging)

**Logger Usage Pattern:**
```python
# In x_client.py
import logging
logger = logging.getLogger('klaus_news.x_client')

async def fetch_posts_from_list(list_id: str):
    logger.info("Fetching posts from X list", extra={'list_id': list_id})

    response = await client.get(url)

    if response.status_code != 200:
        logger.error("X API request failed", extra={
            'status_code': response.status_code,
            'response_body': response.text[:500]
        })
        return []
```

**Critical Bug Fix:**
- **Before:** X API 402 errors silently returned empty list (line 46-47 in x_client.py)
- **After:** Logs error with status code and response body, visible in UI
- This was the primary motivation for implementing the logging system

**Performance Characteristics:**
- Separate connection pool: ~5-10ms overhead per log write
- Doesn't block main request (async write)
- Failed log writes don't crash application
- Batch operations not needed (small per-log overhead)

---

### Settings Service âœ…

**Location:** [backend/app/services/settings_service.py](backend/app/services/settings_service.py)

**Technical Implementation:**

```python
class SettingsService:
    """Load settings from DB with caching"""
    _cache = {}  # Class-level cache
    _cache_expiry = 60  # seconds
    _cache_timestamps = {}

    def get(self, key: str, default=None):
        """Get setting value with caching"""
        # Check if cached and not expired
        if key in self._cache and self._is_cache_fresh(key):
            return self._cache[key]

        # Query database
        setting = db.query(SystemSettings).filter_by(key=key).first()
        if not setting:
            return default

        # Parse value based on type
        value = self._parse_value(setting.value, setting.value_type)

        # Update cache
        self._cache[key] = value
        self._cache_timestamps[key] = time.time()

        return value

    def invalidate_cache(self, key: str = None):
        """Clear cache for key or all keys"""
        if key:
            self._cache.pop(key, None)
            self._cache_timestamps.pop(key, None)
        else:
            self._cache.clear()
            self._cache_timestamps.clear()
```

**How It Works:**
- Class-level cache shared across requests (suitable for single-process deployments)
- 60-second TTL prevents stale reads while reducing database queries
- Automatic type parsing (int, float, bool, json)
- Cache invalidation called from PUT /api/settings/{key} after updates
- Default values prevent crashes if setting missing

**Usage in Scheduler:**
```python
async def ingest_posts_job():
    settings = SettingsService()
    interval = settings.get('ingest_interval_minutes', 30)
    posts_per_fetch = settings.get('posts_per_fetch', 5)
    # Use dynamic settings...
```

**Performance:**
- Cached reads: ~0.1ms (in-memory lookup)
- Cache miss: ~5-10ms (database query + parsing)
- Cache hit rate: >95% in typical usage

---

### Prompt Service âœ…

**Location:** [backend/app/services/prompt_service.py](backend/app/services/prompt_service.py)

**Technical Implementation:**

```python
class PromptService:
    """Load prompts from DB with caching"""
    _cache = {}  # Class-level cache
    _cache_expiry = 300  # 5 minutes (prompts change infrequently)
    _cache_timestamps = {}

    def get_prompt(self, name: str) -> dict:
        """Get prompt configuration with caching"""
        # Check if cached and not expired
        if name in self._cache and self._is_cache_fresh(name):
            return self._cache[name]

        # Query database
        prompt = db.query(Prompt).filter_by(name=name).first()
        if not prompt:
            return self._get_default_prompt(name)  # Fallback to hardcoded defaults

        # Build config dict
        config = {
            'text': prompt.prompt_text,
            'model': prompt.model,
            'temperature': prompt.temperature,
            'max_tokens': prompt.max_tokens
        }

        # Update cache
        self._cache[name] = config
        self._cache_timestamps[name] = time.time()

        return config

    def invalidate_cache(self, name: str = None):
        """Clear cache for prompt or all prompts"""
        if name:
            self._cache.pop(name, None)
            self._cache_timestamps.pop(name, None)
        else:
            self._cache.clear()
            self._cache_timestamps.clear()
```

**How It Works:**
- 5-minute TTL (longer than SettingsService because prompts change less frequently)
- Fallback to hardcoded defaults if database entry missing (safe degradation)
- Cache invalidation called from PUT /api/prompts/{name} after updates
- Used by OpenAI client for all AI operations

**Usage in OpenAI Client:**
```python
def score_worthiness(self, post_text: str, category: str) -> float:
    prompt_svc = PromptService()
    config = prompt_svc.get_prompt('worthiness')
    prompt_text = config['text'].format(post_text=post_text, category=category)
    response = openai.ChatCompletion.create(
        model=config['model'],
        temperature=config['temperature'],
        max_tokens=config['max_tokens'],
        messages=[{"role": "user", "content": prompt_text}]
    )
    # Parse and return score...
```

---

### Teams Service (NEW)

**Location:** backend/app/services/teams_service.py

**Technical Implementation:**

```python
class TeamsService:
    """Multi-channel Teams webhook integration"""

    def get_channels() -> list[dict]:
        """
        Get configured Teams channels from TEAMS_CHANNELS env var.
        Returns list of {name: str} (webhook URLs never exposed).
        """

    def send_to_teams(article_id: str, channel_name: str) -> dict:
        """
        Send article to specified Teams channel.
        Returns {success: bool, message: str} or {success: bool, error: str}
        """

    def build_adaptive_card(article: Article) -> dict:
        """
        Build Microsoft Adaptive Card v1.4 JSON from article data.
        Includes: category badge, title, summary, Read Article button, source footer.
        """
```

**TEAMS_CHANNELS Environment Variable:**
```bash
TEAMS_CHANNELS='[
  {"name": "general-news", "webhookUrl": "https://outlook.office.com/webhook/..."},
  {"name": "tech-updates", "webhookUrl": "https://outlook.office.com/webhook/..."}
]'
```

**Security:**
- Webhook URLs stored in env vars only
- GET /api/teams/channels returns names only (never URLs)
- Input validation on articleId (UUID) and channelName

---

## Frontend Implementation Status

### Components

#### **App.tsx** âœ… FULLY FUNCTIONAL
**Location:** [frontend/src/App.tsx](frontend/src/App.tsx)

**Technical Implementation:**
- Simple root component
- Renders header with "Klaus News" title
- Renders Routes with 3 paths: `/` (Home), `/settings/system` (Settings), `/prompts` (Prompts). Header includes navigation links.
- CSS includes:
  - settings-grid: 3-tile vertical layout (Data Sources, Content Filtering with embedded PromptTiles, System Control)
  - settings-tile: tile styling with overflow-y: auto, max-height: 600px
  - prompts-grid: responsive auto-fill grid (minmax(300px, 1fr))
  - prompt-tile: tile styling with overflow-y: auto, max-height: 400px
  - prompt-tile textarea: overflow handling with max-height: 200px

**Status:** Complete, no changes needed

---

#### **Home.tsx** ðŸŸ¡ FUNCTIONAL BUT INCOMPLETE
**Location:** [frontend/src/pages/Home.tsx](frontend/src/pages/Home.tsx)

**What's Implemented:**
- State management with `useState` hooks:
  - `posts` (array)
  - `view` ('recommended' | 'all')
  - `loading` (boolean)
  - `error` (string)
- `useEffect` hook fetches posts on mount and view change
- Fetches from: groupsApi.getAll() for group-centric display (V-5)
  - `postsApi.getAll()` for all view
- Parses recommended response (flattens category groups)
- Renders view toggle buttons
- Passes posts to PostList component
- Handles group selection: calls `groupsApi.select(id)` (V-18)

**What's Missing:**
- âŒ Navigation after post selection (line 53: TODO comment)
- âŒ No visual feedback on selection
- âŒ Selected post remains visible in list

**Technical Detail:**
- Recommended view returns `{category: [posts]}` dict
- Home.tsx flattens to single array via `Object.values().flat()`

---

#### **PostList.tsx** âœ… FULLY FUNCTIONAL
**Location:** [frontend/src/components/PostList.tsx](frontend/src/components/PostList.tsx)

**Technical Implementation:**
- Receives `groups` array and `onSelectGroup` callback as props (V-18)
- "Write Article" button on group header (not individual posts)
- "Archive" button on group header
- **Group-Centric Display (V-5):** Receives groups array from Home.tsx, displays each group as card with representative_title, post_count badge, and expand/collapse to fetch posts-by-group
  ```typescript
  const seen = new Set();
  const uniquePosts = posts.filter(post => {
    if (post.group_id && seen.has(post.group_id)) {
      return false;
    }
    if (post.group_id) seen.add(post.group_id);
    return true;
  });
  ```
  - Tracks seen group_ids in Set
  - Shows only first post per group_id
  - Collapses duplicates/similar posts
- Renders each post:
  - Title: `ai_title` or fallback "Untitled"
  - Summary: `ai_summary` or fallback to `original_text`
  - Category badge (if available)
  - Worthiness score (if available)
- Click handler: `onClick={() => onSelectPost(post)}`

**Status:** Complete and working perfectly

---

#### **TeamsChannelModal.tsx** (NEW)
**Location:** frontend/src/components/TeamsChannelModal.tsx

**Technical Implementation:**
- Props: `article` (article data), `onClose` (close handler), `onSuccess` (success callback)
- State: `channels` (list), `selectedChannel` (string), `loading` (boolean), `error` (string)
- Fetches channels on mount via `teamsApi.getChannels()`
- Calls `teamsApi.sendToTeams(articleId, channelName)` on submit
- Displays toast notifications for success/error

**UI Elements:**
- Modal overlay with dark background
- Radio button list of channels
- Article preview section (title + summary)
- Cancel and Send to Teams buttons
- Loading state during API call
- Error message display

---

#### **TeamsSettingsSection.tsx** (NEW)
**Location:** frontend/src/components/TeamsSettingsSection.tsx

**Technical Implementation:**
- Fetches channels on mount via `teamsApi.getChannels()`
- Displays channel table with Name and Status columns
- "Test All Connections" button triggers validation
- Empty state with warning icon when no channels
- Info message about environment variable configuration

---

#### **ArticleEditor.tsx** âœ… COMPONENT COMPLETE (Not Integrated)
**Location:** [frontend/src/components/ArticleEditor.tsx](frontend/src/components/ArticleEditor.tsx)

**Technical Implementation:**
- Wraps Quill.js WYSIWYG editor
- Props: `initialContent`, `onChange(content: string)`
- Toolbar modules:
  ```typescript
  modules = {
    toolbar: [
      [{ 'header': [1, 2, 3, false] }],
      ['bold', 'italic', 'underline'],
      ['link'],
      [{ 'list': 'ordered'}, { 'list': 'bullet' }],
      ['clean']
    ]
  }
  ```
- Uses `useRef` to prevent re-initialization
- Calls `onChange` on text-change event with HTML content

**Status:** Ready to use, needs integration into article view page

---

#### **Settings.tsx** âœ… FULLY FUNCTIONAL
**Location:** [frontend/src/pages/Settings.tsx](frontend/src/pages/Settings.tsx)

**Technical Implementation:**
- Vertical layout with 3 tiles: Data Sources, Content Filtering (with embedded PromptTile components), System Control (with Ingestion and Archival sections)
  - Data Sources (DataSourceManager component)
  - Content Filtering (worthiness, duplicate thresholds, Category cards (editable descriptions), Add New Category modal, Category Mismatch Log modal, embedded PromptTile components)
  - System Control with Ingestion section (auto-fetch toggle, interval selector, manual trigger, posts per fetch) and Archival section (age/time settings, manual trigger)

All tiles visible simultaneously. SettingsNav component provides route navigation to `/prompts`.
- State management with `useState` hooks for all setting values
- Real-time validation with inline error messages
- Optimistic updates for instant feedback
- Live preview for threshold changes (shows post count)
- Confirmation dialogs for destructive actions (delete list, archive)
- Color-coded status indicators (green/yellow/red)

**What's Implemented:**
- âœ… Form inputs connected to settings API
- âœ… Save/cancel functionality
- âœ… Reset to defaults button
- âœ… Manual trigger buttons with progress indicators
- âœ… List add/edit/delete with test connectivity
- âœ… Success/error notifications
- âœ… Loading states during API calls

**Status:** Complete and fully functional

---

#### **Prompts.tsx** âœ… FULLY FUNCTIONAL (v2.0 NEW)
**Location:** [frontend/src/pages/Prompts.tsx](frontend/src/pages/Prompts.tsx)

**Technical Implementation:**
- Tile grid of 6 prompts (responsive auto-fill layout):
  - Each prompt rendered as PromptTile component
  - Independent state management per tile
  - Per-tile Save/Reset buttons
  - No sidebar or master-detail pattern
  - SettingsNav component provides route navigation to `/settings/system`

**What's Implemented:**
- âœ… View all prompts with current configurations
- âœ… Edit prompt text and parameters via modal
- âœ… Reset individual prompt to default
- âœ… Export all prompts to JSON
- âœ… Import prompts from JSON (overwrites)
- âœ… Character count for prompt text
- âœ… Validation prevents empty prompts
- âœ… Changes take effect immediately

**Status:** Complete and fully functional

---

#### **SettingsNav.tsx** âœ… FULLY FUNCTIONAL (v2.0 NEW)
**Location:** [frontend/src/components/SettingsNav.tsx](frontend/src/components/SettingsNav.tsx)

**Technical Implementation:**
- Shared navigation component with 2 tabs: "System" and "AI Prompts"
- Uses React Router Link for navigation between `/settings/system` and `/prompts`
- useLocation hook highlights active tab based on current route
- Rendered at top of both Settings and Prompts pages

**Status:** Complete and fully functional

---

#### **PromptTile.tsx** âœ… FULLY FUNCTIONAL (v2.0 NEW)
**Location:** [frontend/src/components/PromptTile.tsx](frontend/src/components/PromptTile.tsx)

**Note:** Reusable component now embedded in Settings page Content Filtering sections (Worthiness, Duplicate Detection, Category Filters) in addition to AI Prompts route.

**Technical Implementation:**
- Individual prompt editor component with own state (prompt_text, model, temperature, max_tokens, description)
- Per-tile Save and Reset buttons with independent API calls
- All 5 fields visible inline (no modal/sidebar)
- Textarea with overflow handling (max-height: 200px)
- Feedback messages for save/reset operations

**Status:** Complete and fully functional

---

#### **DataSourceManager.tsx** âœ… FULLY FUNCTIONAL
**Location:** [frontend/src/components/DataSourceManager.tsx](frontend/src/components/DataSourceManager.tsx)

**Technical Implementation:**
- Table display of X lists with columns:
  - List ID
  - Name (user-friendly, editable)
  - Status (enabled/disabled toggle)
  - Last Fetch (timestamp with color coding)
  - Actions (edit, test, delete)
- Add new list form with:
  - List ID input
  - Test connection button (validates via API)
  - Validation feedback (âœ“ Valid or âœ— Error)
- Enable/disable toggle (affects next ingestion cycle)
- Delete confirmation dialog

**Color Coding Logic:**
```typescript
const getTimestampColor = (timestamp: string) => {
  const ageMinutes = (Date.now() - new Date(timestamp).getTime()) / 60000;
  if (ageMinutes < 30) return 'green';  // Recent
  if (ageMinutes < 60) return 'yellow'; // Stale
  return 'red';  // Very stale
};
```

**Status:** Complete and fully functional

---

#### **LogDetailModal.tsx** âœ… FULLY FUNCTIONAL
**Location:** [frontend/src/components/LogDetailModal.tsx](frontend/src/components/LogDetailModal.tsx)

**Technical Implementation:**
- Modal component for displaying full log details
- Props: `log` (object), `onClose` (callback)
- Displays all log fields:
  - **Basic Info:** Timestamp (formatted), Level (badge), Logger, Category
  - **Message:** Full message in pre-formatted block
  - **Exception Details:** (If error) Type, message, full stack trace
  - **Context:** (If present) JSON context pretty-printed
  - **Correlation ID:** (If present) For tracing related logs
- Stack traces displayed in monospace font with scrolling
- Close button closes modal and clears selection

**Layout:**
```typescript
<div className="modal-overlay" onClick={onClose}>
  <div className="modal-content modal-large" onClick={(e) => e.stopPropagation()}>
    <div className="modal-header">
      <h2>Log Details</h2>
      <button className="modal-close">Ã—</button>
    </div>
    <div className="modal-body">
      {/* Log detail sections */}
    </div>
    <div className="modal-footer">
      <button className="btn-primary">Close</button>
    </div>
  </div>
</div>
```

**Status:** Complete and fully functional

---

### API Client âœ…

**Location:** [frontend/src/services/api.ts](frontend/src/services/api.ts)

**Technical Implementation:**

```typescript
const apiClient = axios.create({
  baseURL: import.meta.env.VITE_API_URL || 'http://localhost:8000',
  headers: { 'Content-Type': 'application/json' }
});

export const postsApi = {
  getAll: () => apiClient.get<PostsResponse>('/api/posts'),
  getRecommended: () => apiClient.get<PostsResponse>('/api/posts/recommended'),
  getById: (id: number) => apiClient.get<PostResponse>(`/api/posts/${id}`)
  // selectPost removed - selection now at group level (V-15)
};

export const teamsApi = {
  getChannels: () => apiClient.get('/api/teams/channels'),
  sendToTeams: (articleId: string, channelName: string) =>
    apiClient.post('/api/teams/send', { articleId, channelName })
};

export const groupsApi = {
  getAll: () => apiClient.get<GroupsResponse>('/api/groups/'),
  getArchived: () => apiClient.get<GroupsResponse>('/api/groups/archived/'),
  getPostsByGroup: (id: number) => apiClient.get(`/api/groups/${id}/posts/`),
  select: (id: number) => apiClient.post(`/api/groups/${id}/select/`),
  archive: (id: number) => apiClient.post(`/api/groups/${id}/archive/`),
  unarchive: (id: number) => apiClient.post(`/api/groups/${id}/unarchive/`)
};

export const articlesApi = {
  getAll: () => apiClient.get<ArticlesResponse>('/api/articles'),
  create: (postId: number) => apiClient.post('/api/articles', { post_id: postId }),
  update: (id: number, content: string) =>
    apiClient.put(`/api/articles/${id}`, { content }),
  regenerate: (id: number) => apiClient.post(`/api/articles/${id}/regenerate`),
  postToTeams: (id: number) => apiClient.post(`/api/articles/${id}/post-to-teams`)
};

export const settingsApi = {
  getAll: () => apiClient.get<SettingsResponse>('/api/settings/'),
  getByKey: (key: string) => apiClient.get<SettingResponse>(`/api/settings/${key}`),
  update: (key: string, value: any) =>
    apiClient.put(`/api/settings/${key}`, { value }),
  batchUpdate: (settings: Array<{key: string, value: any}>) =>
    apiClient.post('/api/settings/batch', { settings }),
  reset: () => apiClient.post('/api/settings/reset'),
  validate: (key: string, value: any) =>
    apiClient.get(`/api/settings/validate/${key}`, { params: { value } })
};

export const listsApi = {
  getAll: () => apiClient.get<ListsResponse>('/api/lists/'),
  create: (listId: string, name?: string) =>
    apiClient.post('/api/lists/', { list_id: listId, list_name: name }),
  update: (id: number, data: {enabled?: boolean, list_name?: string, description?: string}) =>
    apiClient.put(`/api/lists/${id}`, data),
  delete: (id: number) => apiClient.delete(`/api/lists/${id}`),
  test: (id: number) => apiClient.post(`/api/lists/${id}/test`),
  getStats: (id: number) => apiClient.get(`/api/lists/${id}/stats`)
};

export const adminApi = {
  triggerIngest: () => apiClient.post('/api/admin/trigger-ingest'),
  triggerArchive: () => apiClient.post('/api/admin/trigger-archive'),
  pauseScheduler: () => apiClient.post('/api/admin/pause-scheduler'),
  resumeScheduler: () => apiClient.post('/api/admin/resume-scheduler'),
  getSchedulerStatus: () => apiClient.get('/api/admin/scheduler-status'),
  getSystemStats: () => apiClient.get('/api/admin/system-stats')
};

export const logsApi = {
  getAll: (params: {
    level?: string;
    category?: string;
    hours?: number;
    limit?: number;
    offset?: number;
  }) => apiClient.get('/api/logs/', { params }),
  getStats: (hours: number = 24) => apiClient.get(`/api/logs/stats?hours=${hours}`),
  getById: (id: number) => apiClient.get(`/api/logs/${id}`),
  cleanup: (days: number = 7) => apiClient.delete(`/api/logs/cleanup?days=${days}`)
};
```

**Technical Details:**
- Uses environment variable for API URL (Vite: `VITE_API_URL`)
- All endpoints typed with TypeScript interfaces
- Returns Axios promises (async/await compatible)
- Currently used: `postsApi.getAll()`, `postsApi.getRecommended()`, `postsApi.selectPost()`, `logsApi.*`
- Unused but defined: All article endpoints (ready for integration)

**Status:** Complete, ready for article workflow integration

---

### Type Definitions âœ…

**Location:** [frontend/src/types/index.ts](frontend/src/types/index.ts)

**Technical Implementation:**

```typescript
export interface Post {
  id: number;
  post_id: string;
  original_text: string;
  author: string | null;
  created_at: string;
  ai_title: string | null;
  ai_summary: string | null;
  category: string | null;
  categorization_score: number | null;
  worthiness_score: number | null;
  group_id: string | null;
  archived: boolean;
  selected: boolean;
  ingested_at: string;
  updated_at: string;
}

export interface Article {
  id: number;
  post_id: number;
  title: string;
  content: string;
  research_summary: string | null;
  generation_count: number;
  posted_to_teams: string | null;
  created_at: string;
  updated_at: string;
}

export interface Setting {
  id: number;
  key: string;
  value: string;
  value_type: 'int' | 'float' | 'string' | 'bool' | 'json';
  description: string | null;
  category: 'scheduling' | 'filtering' | 'system';
  min_value: number | null;
  max_value: number | null;
  updated_at: string;
  updated_by: string | null;
}

export interface ListMetadata {
  id: number;
  list_id: string;
  last_tweet_id: string | null;
  enabled: boolean;
  list_name: string | null;
  description: string | null;
  created_at: string;
  updated_at: string;
}

export interface Group {
  id: number;
  representative_title: string;
  category: string;
  first_seen: string;
  post_count: number;
}
```

**Status:** Complete, matches backend models exactly

---

## Backup & Restore Scripts âœ…

**Location:** Root directory ([backup_db.sh](backup_db.sh), [restore_db.sh](restore_db.sh))

**Technical Implementation:**

### backup_db.sh âœ…
```bash
#!/bin/bash
set -e

TIMESTAMP=$(date +%Y%m%d_%H%M%S)
BACKUP_DIR="./backups"
BACKUP_FILE="${BACKUP_DIR}/klaus_news_backup_${TIMESTAMP}.sql"

mkdir -p "${BACKUP_DIR}"
docker-compose exec -T postgres pg_dump -U postgres klaus_news > "${BACKUP_FILE}"
```

**How It Works:**
- Uses `docker-compose exec -T` to run pg_dump inside postgres container
- `-T` flag disables pseudo-TTY allocation (required for piping output)
- Stores backups outside Docker volumes in `./backups/` directory
- Timestamped filenames prevent overwrites
- Backups survive `docker-compose down -v`

### restore_db.sh âœ…
**How It Works:**
- Validates backup file exists before proceeding
- Prompts user for confirmation (shows warning about data overwrite)
- Stops backend/frontend containers during restore (prevents connection conflicts)
- Drops existing database and recreates from backup
- Uses `docker-compose exec -T postgres psql` with input redirection
- Restarts containers after successful restore

**Technical Details:**
- Requires postgres container to be running
- Uses database credentials from .env file (via docker-compose)
- Error handling with `set -e` (exit on any command failure)

---

## Configuration Management âœ…

**Location:** [backend/app/config.py](backend/app/config.py)

**Technical Implementation:**

```python
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    x_api_key: str = "placeholder_x_api_key"
    x_api_secret: str = "placeholder_x_api_secret"
    x_list_ids: str = "placeholder_list_id_1,placeholder_list_id_2,placeholder_list_id_3"
    openai_api_key: str = "placeholder_openai_api_key"
    teams_webhook_url: str = "placeholder_teams_webhook_url"
    database_url: str = "postgresql://postgres:postgres@postgres:5432/klaus_news"
    app_name: str = "Klaus News"
    debug: bool = True

    def get_x_list_ids(self) -> list[str]:
        """Parse X_LIST_IDS from comma-separated string to list"""
        return [lid.strip() for lid in self.x_list_ids.split(",") if lid.strip()]

    class Config:
        env_file = ".env"
```

**How It Works:**
- Pydantic BaseSettings auto-loads from `.env` file
- Defaults provided for local development
- `get_x_list_ids()` helper method parses comma-separated lists
- Validates types automatically (Pydantic)

**Environment Variables Required:**
- `X_API_KEY` - X/Twitter API Bearer token
- `X_LIST_IDS` - Comma-separated X list IDs (e.g., "123,456,789")
- `OPENAI_API_KEY` - OpenAI API key
- `TEAMS_CHANNELS` - JSON array of Teams channels: `[{"name": "channel", "webhookUrl": "https://..."}]`
- `DATABASE_URL` - PostgreSQL connection string

**Template:** `.env.example` provided in repository

---

## Docker Configuration âœ…

### Services

**docker-compose.yml** defines 3 services:

1. **postgres**: PostgreSQL 15 with health checks
2. **backend**: FastAPI with hot reload (port 8000)
3. **frontend**: Vite dev server (port 3000) or Nginx in production (port 80)

**Location:** [docker-compose.yml](docker-compose.yml)

### Dockerfiles

- **backend/Dockerfile**: Multi-stage Python build
- **frontend/Dockerfile**: Multi-stage Node build with Nginx

---

## Technical Architecture Decisions

### Problem 1: Refetch Prevention âœ… SOLVED

**Requirement:** Don't refetch same tweets across ingestion cycles

**Solution - Two Layers of Protection:**

#### Layer 1: X API `since_id` Parameter (Primary)
- **Location:** [x_client.py:31-32](backend/app/services/x_client.py#L31-L32)
- Pass `last_tweet_id` as `since_id` to X API
- X API returns only tweets posted AFTER this ID (non-inclusive)
- Prevents fetching already-seen tweets at the API level

#### Layer 2: Database Duplicate Check (Secondary)
- **Location:** [scheduler.py:52-54](backend/app/services/scheduler.py#L52-L54)
- Before processing each post, check if `post_id` already exists in database
- Skip if duplicate found (handles edge cases and cross-list duplicates)

#### Per-List Independent Tracking

The `list_metadata` table stores **one record per list**, each with its own pagination state:

```
| list_id     | last_tweet_id      | enabled |
|-------------|--------------------| --------|
| 123456789   | 1879234567890123   | true    |  â† fetched recently
| 987654321   | 1879198765432109   | true    |  â† fetched recently
| 555555555   | null               | true    |  â† newly added list
```

**Update Mechanism:**
- After each successful fetch, update `last_tweet_id = max(fetched_post_ids)`
- **Location:** [scheduler.py:76-78](backend/app/services/scheduler.py#L76-L78)
- Persists across application restarts

#### New List Behavior

When a new list is added:
1. `last_tweet_id` starts as `null`
2. First fetch has no `since_id` filter â†’ returns most recent tweets (up to `posts_per_fetch`)
3. After first fetch, `last_tweet_id` is set â†’ subsequent fetches only get newer tweets

#### Manual Trigger Behavior ("Fetch 50 Posts NOW")

The manual trigger (`POST /api/admin/trigger-ingestion`) calls the **same** `ingest_posts_job()` function as the scheduler. This means:

| List State | `since_id` Value | What Gets Fetched |
|------------|------------------|-------------------|
| **New list** (just added) | `null` | Up to N most recent tweets (no filter) |
| **Recently fetched** (minutes ago) | Set to recent ID | Only tweets **newer** than that ID (likely 0-5) |
| **Stale list** (days since fetch) | Set to old ID | All tweets since that old ID (up to N) |

**Key Implications:**
- `posts_per_fetch` is **per list**, not total across all lists
- If set to 50 with 3 lists, you could get up to 150 posts total (50 Ã— 3)
- In practice, recently-fetched lists return fewer posts due to `since_id` filtering
- New lists get a full initial fetch since `since_id` is null

#### Cross-List Duplicate Handling

If the **same tweet** appears in multiple X lists (e.g., a user is in two lists):
- The `since_id` filter won't catch this (different lists have different positions)
- The database-level check on `post_id` prevents inserting the same tweet twice
- **Location:** [scheduler.py:82-87](backend/app/services/scheduler.py#L82-L87)

#### Important Gotcha: Scheduler Paused State

**Warning:** If `scheduler_paused=true`, even manual triggers will skip execution.
- **Location:** [scheduler.py:51-52](backend/app/services/scheduler.py#L51-L52)
- The `ingest_posts_job()` checks the pause flag and returns early if paused
- This may be unexpected - users might want manual triggers to work even when auto-fetch is paused

**Technical Advantages:**
- Leverages X API's native pagination (efficient, no wasted API calls)
- No client-side filtering needed for primary deduplication
- Stateful across process restarts (persisted in database)
- Independent tracking per list (adding/removing lists doesn't affect others)
- Secondary database check catches edge cases and cross-list duplicates

---

### Problem 2: Topic Grouping âœ… SOLVED

**Requirement:** Group duplicate/similar posts together

**Solution: AI Semantic Title Comparison**

During ingestion, posts are grouped using AI to compare AI-generated titles:

1. **Generate Title:** Each new post gets an AI-generated title
2. **Find Candidates:** Query posts with same category, last 7 days (limit 50)
3. **AI Comparison:** Compare new title vs each candidate title using GPT-5-mini
4. **Threshold Check:** If AI similarity score >= `duplicate_threshold` â†’ group together
5. **Group Assignment:** Either join existing group or create new one

**Configuration:**
- `duplicate_threshold` setting (default: 0.85) controls sensitivity
- Adjustable in Settings UI under "Duplicate Detection"
- Lower = more aggressive grouping, Higher = stricter matching

**Technical Advantages:**
- AI understands semantic meaning (catches rephrased content)
- Cost-optimized via category filtering and candidate limits
- Configurable sensitivity via threshold setting
- Uses cost-effective GPT-5-mini model

**Frontend Integration:**
- PostList.tsx deduplicates by group_id
- Shows only first post per group
- Users see consolidated view automatically

---

## What Is Implemented vs Placeholder

### âœ… FULLY IMPLEMENTED (Production-Ready):

**Backend:**
- X API integration (fetch posts from lists)
- OpenAI integration (categorization, titles, summaries, articles)
- Duplicate detection (AI semantic title comparison)
- Scoring algorithm (worthiness calculation)
- Background scheduler (ingest every 30 min, archive daily, log cleanup daily)
- Teams webhook posting (Adaptive Cards)
- Post archival (7-day cutoff)
- Database models (all tables including system_logs)
- API endpoints (28 fully functional: 4 posts, 5 articles, 6 settings, 6 lists, 5 admin, 4 logs)
- Settings management system (SettingsService with caching)
- Dynamic scheduler rescheduling (no restart required)
- List management (enable/disable, test connectivity)
- Production logging system (database persistence, 7-day retention, UI visibility)
- Custom logging handler (DatabaseLogHandler with separate connection pool)
- Comprehensive error tracking (X API 402 errors, OpenAI failures, scheduler errors)

**Frontend:**
- Post browsing (recommended + all views)
- View toggle
- Post display with AI metadata
- Duplicate collapsing (group_id)
- Settings page (complete with all sections including logs)
- Data source management (list add/edit/delete/test)
- Scheduling controls (intervals, archival, posts per fetch)
- Content filtering (AI thresholds, category toggles)
- System control (manual triggers, scheduler pause/resume)
- System logs (view, filter, detail modal, cleanup)
- API client (all endpoints defined and functional including logs)
- Type safety (complete TypeScript types)
- ArticleEditor component (Quill.js)
- LogDetailModal component (full log details with stack traces)

---

### ðŸŸ¡ PARTIALLY IMPLEMENTED:

**Post Selection:**
- Backend: âœ… Marks post as selected
- Frontend: âœ… Calls API
- Frontend: âŒ No navigation after selection

---

### âŒ NOT IMPLEMENTED:

**Backend:**
- Research functionality (field exists, logic not implemented)
- `GET /api/posts/{id}` endpoint (returns null)
- `GET /api/articles` endpoint (returns empty)

**Frontend:**
- Article generation view/page
- Article editor integration
- Navigation/routing (React Router)
- Article list/management page
- User feedback (notifications, confirmations)
- Mobile responsive design

---

## Technical Gaps & Next Steps

### High Priority (Complete Article Workflow):

1. **Add React Router** (0.5 day)
   - Install: `react-router-dom`
   - Define routes: `/`, `/article/:id`, `/articles`
   - Implement navigation on post selection

2. **Create ArticleView Page** (1 day)
   - Component structure:
     ```
     ArticleView
     â”œâ”€â”€ ArticleEditor (already built)
     â”œâ”€â”€ Action buttons (regenerate, post to Teams)
     â””â”€â”€ Status indicators (loading, success)
     ```
   - API integration:
     - On mount: `articlesApi.create(postId)` â†’ generate article
     - On edit: `articlesApi.update(id, content)` â†’ save changes
     - On regenerate click: `articlesApi.regenerate(id)` â†’ new version
     - On Teams click: `articlesApi.postToTeams(id)` â†’ post

3. **Add User Feedback** (0.5 day)
   - Install: `react-toastify` or similar
   - Success messages: "Article generated", "Posted to Teams"
   - Error handling: "Failed to generate", "Teams post failed"
   - Loading states: Spinners during API calls

### Medium Priority (Polish):

4. **Complete Placeholder Endpoints** (1 hour)
   - `GET /api/posts/{id}` - Query by ID, return Post
   - `GET /api/articles` - Query all articles, return list

5. **Article List Page** (0.5 day)
   - Display all created articles
   - Show posted_to_teams status
   - Link to edit existing articles

### Low Priority (Future):

6. **Research Functionality** (2-3 days)
   - Integrate web search API
   - Summarize research via OpenAI
   - Populate research_summary field

7. **Mobile Responsive Design** (1-2 days)
   - CSS breakpoints
   - Touch-optimized UI
   - Mobile layout adjustments

---

## Performance Considerations

### Backend:

**Strengths:**
- Async/await throughout (non-blocking I/O)
- Database indexes on frequently queried fields
- Batch operations in scheduler
- AI duplicate detection with category filtering and candidate limits

**Potential Bottlenecks:**
- AI duplicate detection scales with candidate count
  - Mitigation: Limited to 50 candidates per post, filtered by category and time
  - Cost control: Uses GPT-4o-mini (cost-effective model)
- OpenAI API calls (rate limits)
  - Current: 3+ calls per post (categorize, title, summary, duplicate checks)
  - Duplicate threshold setting controls how many comparisons are needed

### Frontend:

**Strengths:**
- Vite fast refresh
- Component-based architecture
- TypeScript type safety

**Potential Improvements:**
- Add React Query for caching
- Implement pagination (currently loads all posts)
- Memoize PostList renders

---

## Development Commands

```bash
# Start all services
docker-compose up -d

# View logs
docker-compose logs -f backend
docker-compose logs -f frontend

# Access database
docker-compose exec postgres psql -U postgres -d klaus_news

# Run backend tests (when implemented)
docker-compose exec backend pytest

# Run frontend tests (when implemented)
docker-compose exec frontend npm test

# Stop all services
docker-compose down

# Reset database
docker-compose down -v
docker-compose up -d
```

---

## File Structure

```
klaus-news/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py              # FastAPI app entry âœ…
â”‚   â”‚   â”œâ”€â”€ config.py            # Settings âœ…
â”‚   â”‚   â”œâ”€â”€ database.py          # SQLAlchemy setup âœ…
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ post.py          # Post model âœ…
â”‚   â”‚   â”‚   â”œâ”€â”€ article.py       # Article model âœ…
â”‚   â”‚   â”‚   â”œâ”€â”€ list_metadata.py # ListMetadata model âœ…
â”‚   â”‚   â”‚   â”œâ”€â”€ group.py          # Group model âœ… (V-4)
â”‚   â”‚   â”‚   â”œâ”€â”€ system_log.py     # SystemLog model âœ… (Production logging)
â”‚   â”‚   â”‚   â”œâ”€â”€ group_research.py  # GroupResearch model âœ… (V-18) TBD
â”‚   â”‚   â”‚   â””â”€â”€ group_articles.py  # GroupArticles model âœ… (V-18) TBD
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ x_client.py      # X API client âœ…
â”‚   â”‚   â”‚   â”œâ”€â”€ openai_client.py # OpenAI client (includes duplicate detection) âœ…
â”‚   â”‚   â”‚   â”œâ”€â”€ teams_service.py # Teams multi-channel webhook âœ…
â”‚   â”‚   â”‚   â”œâ”€â”€ scheduler.py     # Background jobs (3 jobs: ingest, archive, log cleanup) âœ…
â”‚   â”‚   â”‚   â”œâ”€â”€ settings_service.py # Settings with caching âœ…
â”‚   â”‚   â”‚   â”œâ”€â”€ logging_handler.py  # Custom database logging handler âœ…
â”‚   â”‚   â”‚   â””â”€â”€ logging_config.py   # Logging setup âœ…
â”‚   â”‚   â””â”€â”€ api/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ posts.py         # Posts endpoints âœ…
â”‚   â”‚       â”œâ”€â”€ articles.py      # Articles endpoints âœ…
â”‚   â”‚       â”œâ”€â”€ settings.py      # Settings endpoints âœ…
â”‚   â”‚       â”œâ”€â”€ lists.py         # Lists endpoints âœ…
â”‚   â”‚       â”œâ”€â”€ admin.py         # Admin endpoints âœ…
â”‚   â”‚       â”œâ”€â”€ groups.py        # Groups endpoints âœ… (V-5)
â”‚   â”‚       â”œâ”€â”€ logs.py          # Logs endpoints âœ… (Production monitoring)
â”‚   â”‚       â”œâ”€â”€ research.py      # Research endpoints âœ… (V-19) TBD
â”‚   â”‚       â””â”€â”€ group_articles.py # Article generation endpoints âœ… (V-19) TBD
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ .dockerignore
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ main.tsx             # React entry âœ…
â”‚   â”‚   â”œâ”€â”€ App.tsx              # Main component âœ…
â”‚   â”‚   â”œâ”€â”€ types/
â”‚   â”‚   â”‚   â””â”€â”€ index.ts         # TypeScript types âœ…
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ PostList.tsx     # Post display âœ…
â”‚   â”‚   â”‚   â”œâ”€â”€ SettingsNav.tsx  # Shared settings navigation âœ…
â”‚   â”‚   â”‚   â”œâ”€â”€ PromptTile.tsx   # Individual prompt editor tile âœ…
â”‚   â”‚   â”‚   â”œâ”€â”€ ArticleEditor.tsx # WYSIWYG editor âœ…
â”‚   â”‚   â”‚   â”œâ”€â”€ DataSourceManager.tsx # List management âœ…
â”‚   â”‚   â”‚   â”œâ”€â”€ LogDetailModal.tsx # Log detail viewer âœ…
â”‚   â”‚   â”‚   â””â”€â”€ ManualOperations.tsx  # Admin controls âœ…
â”‚   â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”‚   â”œâ”€â”€ Home.tsx         # Home page ðŸŸ¡
â”‚   â”‚   â”‚   â”œâ”€â”€ Settings.tsx     # System Settings page (2Ã—2 grid) âœ…
â”‚   â”‚   â”‚   â””â”€â”€ Prompts.tsx      # AI Prompts page (tile grid) âœ…
â”‚   â”‚   â””â”€â”€ services/
â”‚   â”‚       â””â”€â”€ api.ts           # Axios client âœ…
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ tsconfig.json
â”‚   â”œâ”€â”€ vite.config.ts
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ nginx.conf
â”‚   â””â”€â”€ .dockerignore
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ TECH_OVERVIEW.md         # This file
â”‚   â”œâ”€â”€ USER_JOURNEY.md          # User perspective
â”‚   â”œâ”€â”€ IMPLEMENTATION_COVERAGE_REPORT.md # Gap analysis
â”‚   â”œâ”€â”€ new-brief.md             # Requirements
â”‚   â””â”€â”€ new-brief2.md            # Requirements (duplicate)
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

**Legend:**
- âœ… Fully implemented
- ðŸŸ¡ Partially implemented
- âŒ Not implemented

---

**Document Status:** Reflects actual technical implementation - ~95% backend complete, ~75% frontend complete (Settings feature fully implemented)
